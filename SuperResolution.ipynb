{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "SuperResolution.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dszZp3-0TlDX"
      },
      "source": [
        "# Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network\n",
        "\n",
        "In this notebook we reproduce some results of the Super Resolution paper [1] in PyTorch.\n",
        "\n",
        "[1] Ledig, C., Theis, L., Husz√°r, F., Caballero, J., Cunningham, A., Acosta, A., ... & Shi, W. (2017). Photo-realistic single image super-resolution using a generative adversarial network. In _Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 4681-4690)_."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Mi7a5iu6vHg6"
      },
      "source": [
        "## Pre-processing: Loading datasets\n",
        "In a first step we download all the required datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IB7wrWhfTkwh",
        "pycharm": {
          "is_executing": true
        },
        "colab": {}
      },
      "source": [
        "import requests\n",
        "\n",
        "url_91 = (\"https://drive.google.com/uc?export=download&id=1eVfd2Snh5bCl0ulMsRE4ker_p-o1M_lm\")\n",
        "url_set5 = (\"https://drive.google.com/uc?export=download&id=1Cr4puJ1UpkXrGpzdpqZLNhZiZ2vaimoi\")\n",
        "url_set14 = (\"https://drive.google.com/uc?export=download&id=1PQus6Glc3VsfVIywG6MAMBBBZVyyF_gB\")\n",
        "url_BSD300 = (\"https://github.com/jorisquist/DLSuperResolutionProject/raw/master/BSD300.zip\")\n",
        "url_BSD500 = (\"https://github.com/jorisquist/DLSuperResolutionProject/raw/master/BSD500.zip\")\n",
        "url_SuperTexture = (\"https://github.com/jorisquist/DLSuperResolutionProject/raw/master/SuperTexture.zip\")\n",
        "\n",
        "\n",
        "# Download data from Google drive and store as zip.\n",
        "def download_url(url, save_path, chunk_size=128):\n",
        "    r = requests.get(url, stream=True)\n",
        "    with open(save_path, \"wb\") as fd:\n",
        "        for chunk in r.iter_content(chunk_size=chunk_size):\n",
        "            fd.write(chunk)\n",
        "\n",
        "\n",
        "download_url(url_91, \"./91.zip\")\n",
        "download_url(url_set5, \"./set5.zip\")\n",
        "download_url(url_set14, \"./set14.zip\")\n",
        "download_url(url_BSD300, \"./BSD300.zip\")\n",
        "download_url(url_BSD500, \"./BSD500.zip\")\n",
        "download_url(url_SuperTexture, \"./SuperTexture.zip\")\n",
        "\n",
        "from zipfile import ZipFile\n",
        "\n",
        "with ZipFile(\"91.zip\", \"r\") as zipObj:\n",
        "    zipObj.extractall(\"./train_data\")\n",
        "\n",
        "with ZipFile(\"set5.zip\", \"r\") as zipObj:\n",
        "    zipObj.extractall(\"./test_data\")\n",
        "\n",
        "with ZipFile(\"set14.zip\", \"r\") as zipObj:\n",
        "    zipObj.extractall(\"./test_data\")\n",
        "\n",
        "with ZipFile(\"BSD300.zip\", \"r\") as zipObj:\n",
        "    zipObj.extractall(\"./test_data\")\n",
        "\n",
        "with ZipFile(\"BSD500.zip\", \"r\") as zipObj:\n",
        "    zipObj.extractall(\"./test_data\")\n",
        "\n",
        "with ZipFile(\"SuperTexture.zip\", \"r\") as zipObj:\n",
        "    zipObj.extractall(\"./test_data\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKD73Y5EBLHC",
        "colab_type": "text"
      },
      "source": [
        "## Pre-processing: Prepare trainingset\n",
        "We load in the training set using our custom loader. This loader also up/downscales the images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "RAAvHmo_BLHE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from skimage import io\n",
        "from os import listdir\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL\n",
        "\n",
        "\n",
        "class SuperResolutionDataset(Dataset):\n",
        "    def __init__(self, root_dir, upscale_factor, use_gpu=False, testing=False):\n",
        "        self.testing = testing \n",
        "        self.use_gpu = use_gpu\n",
        "        self.root_dir = root_dir\n",
        "        self.upscale_factor = upscale_factor\n",
        "        self.images = [\n",
        "            f\n",
        "            for f in listdir(self.root_dir)\n",
        "            if f.endswith(\".bmp\") or f.endswith(\".jpg\")\n",
        "        ]\n",
        "        self.data = list()\n",
        "        for image_name in self.images:\n",
        "            self.data = self.data + self.get_data_from_image(image_name)\n",
        "            \n",
        "        if use_gpu:\n",
        "            for i in range(len(self.data)):\n",
        "                self.data[i] = (self.data[i][0].cuda(), self.data[i][1].cuda())\n",
        "                \n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return self.data[item]\n",
        "\n",
        "    def get_data_from_image(self, image_name):\n",
        "        image = io.imread(self.root_dir + \"/\" + image_name)\n",
        "\n",
        "        h, w = len(image), len(image[0])\n",
        "        cropped_h = h - (h % self.upscale_factor)\n",
        "        cropped_w = w - (w % self.upscale_factor)\n",
        "\n",
        "        target_transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.CenterCrop([cropped_h, cropped_w]),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "        input_transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.CenterCrop([cropped_h, cropped_w]),\n",
        "            transforms.Resize(\n",
        "                [\n",
        "                    int(cropped_h // self.upscale_factor),\n",
        "                    int(cropped_w // self.upscale_factor),\n",
        "                ],\n",
        "                PIL.Image.BICUBIC,\n",
        "            ),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "        input_tensor = input_transform(image)\n",
        "        target_tensor = target_transform(image)\n",
        "\n",
        "\n",
        "        if self.testing:\n",
        "          data = list()\n",
        "          data.append((input_tensor, target_tensor))\n",
        "          return data\n",
        "\n",
        "        else:\n",
        "          target_size = 17*self.upscale_factor # patch size\n",
        "          target_stride = 17*self.upscale_factor # patch stride\n",
        "          target_patches = target_tensor.unfold(1, target_size, target_stride).unfold(2, target_size, target_stride)\n",
        "\n",
        "          input_patches = input_tensor.unfold(1, 17, 17).unfold(2, 17, 17)\n",
        "          \n",
        "          data = list()\n",
        "          # target_image = patches[:, 0, 0, :, :]\n",
        "          for i in range(input_patches.shape[1]):\n",
        "            for j in range(input_patches.shape[2]):\n",
        "\n",
        "              # During training we can skip the deconvolution layer if we do a reverse deconvolution on the targets\n",
        "              if not self.testing:\n",
        "                target = target_patches[:, i, j, :, :]\n",
        "                target = target.unsqueeze(0)\n",
        "                target = self.inverse_deconvolution(target, self.upscale_factor)\n",
        "                target = target.squeeze()\n",
        "\n",
        "              data.append((input_patches[:, i, j, :, :], target))\n",
        "\n",
        "          return data\n",
        "\n",
        "    def inverse_deconvolution(self, x, r):\n",
        "      [B, C, H, W] = list(x.size())\n",
        "      x = x.reshape(B, C, H//r, r, W//r, r)\n",
        "      x = x.permute(0, 1, 3, 5, 2, 4)\n",
        "      x = x.reshape(B, C*(r**2), H//r, W//r)\n",
        "      return x\n",
        "\n",
        "    def imshow(self, img):\n",
        "        img = torchvision.utils.make_grid(img)\n",
        "        npimg = img.numpy()\n",
        "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "        plt.show()\n",
        "\n",
        "    def imshow_input(self, idx):\n",
        "        img, _ = self.__getitem__(idx)\n",
        "        img = torchvision.utils.make_grid(img)\n",
        "        npimg = img.numpy()\n",
        "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "        plt.show()\n",
        "\n",
        "    def imshow_target(self, idx):\n",
        "        _, img = self.__getitem__(idx)\n",
        "        img = torchvision.utils.make_grid(img)\n",
        "        npimg = img.numpy()\n",
        "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "        plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12AlbJGwBLHL",
        "colab_type": "text"
      },
      "source": [
        "The goal of this Deep Learning network is to upscale the image resolution. Therefore for our training set we first downscale the input. In the example below you see the input image for different upscale factors (4 and 3 respectively) whereas the target image is the image we want to retrieve in the end."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "LqAVTkQXBLHN",
        "colab_type": "code",
        "outputId": "e1471f2c-ef15-4bb6-8ac8-c2b0ae5978e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        }
      },
      "source": [
        "trainExample4 = SuperResolutionDataset(\"train_data/Set91\", 4)\n",
        "print(\"Input image (Up-scale factor 4): \")\n",
        "trainExample4.imshow_input(0)\n",
        "\n",
        "trainExample3 = SuperResolutionDataset(\"train_data/Set91\", 3)\n",
        "print(\"Input image (Up-scale factor 3): \")\n",
        "trainExample3.imshow_input(0)"
      ],
      "execution_count": 272,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input image (Up-scale factor 4): \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Input image (Up-scale factor 3): \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "ApjK2cmFBLHT",
        "colab_type": "code",
        "outputId": "cab0dfe7-36fb-491d-aeae-8882aaae3cb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        }
      },
      "source": [
        "print(\"Target image: \")\n",
        "trainExample3.imshow_target(0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Target image: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYr4OXxQBLHY",
        "colab_type": "text"
      },
      "source": [
        "## Network Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "ZnrN7VNRBLHZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class SuperResolutionNet(nn.Module):\n",
        "    def __init__(self, r, activation=nn.Identity()):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        self.activation = activation\n",
        "\n",
        "        self.deconvolution = nn.PixelShuffle(self.r)\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            nn.Conv2d(3, 64, 5, padding=2),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),\n",
        "            nn.Conv2d(64, 32, 3, padding=1),\n",
        "        ])\n",
        "\n",
        "        self.last_layer = nn.Conv2d(32, self.r * self.r * 3, 3, padding=1)\n",
        "\n",
        "        # self.params = list(self.layers.parameters())\n",
        "\n",
        "        self.l = len(self.layers) - 1  # The number of hidden layers\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = self.activation(layer(x))\n",
        "\n",
        "        x = self.last_layer(x)  # Don't use the activation on the last convolutional layer\n",
        "\n",
        "        if not self.training:\n",
        "          x = self.deconvolution(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIRe7RhFBLHg",
        "colab_type": "text"
      },
      "source": [
        "## Training Network\n",
        "Function to train a certain network with a certain data loader and hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfFaDMHRBLHh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "import math\n",
        "import datetime\n",
        "\n",
        "from torch import optim\n",
        "from . import SuperResolutionNet\n",
        "\n",
        "def mse_to_psnr(mse):\n",
        "    return 10 * math.log10(1. / mse)\n",
        "\n",
        "def train(net, use_gpu, train_loader, r,\n",
        "          learning_rate=0.01,\n",
        "          max_epochs_without_improvement=100,\n",
        "          max_epochs=30000,\n",
        "          print_output=True,\n",
        "          momentum=0.9,\n",
        "          learning_rate_factor=0.99,\n",
        "          learning_rate_abs=0.00001,\n",
        "          beta1=0.9,\n",
        "          beta2=0.999):\n",
        "\n",
        "    # Decide to use GPU or not.\n",
        "    if use_gpu:\n",
        "        net = net.cuda()\n",
        "        if print_output:\n",
        "          print('Running on gpu')\n",
        "\n",
        "    # Set loss function and optimizer.\n",
        "    loss_function = nn.MSELoss()\n",
        "\n",
        "    optimizer = optim.Adam(\n",
        "    [\n",
        "        {\"params\": net.layers.parameters()},\n",
        "        {\"params\": net.last_layer.parameters(), \"lr\": learning_rate/10},\n",
        "    ],\n",
        "    lr=learning_rate,\n",
        ")\n",
        "\n",
        "    # Save computer name to use when storing network\n",
        "    computer_name = \"unknown\"\n",
        "    try:\n",
        "      computer_name = os.environ['COMPUTERNAME']\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "    # Initialize loss.\n",
        "    lowest_loss = (0, float('inf'))\n",
        "    previous_loss = float('inf')\n",
        "    highest_psnr = - float('inf')\n",
        "\n",
        "    begin_time = time.time()\n",
        "    minimum_psnr_to_save = 20\n",
        "\n",
        "    # Start training.\n",
        "    for epoch in range(max_epochs):\n",
        "        train_loss = []\n",
        "\n",
        "        # Train and propagate through network.\n",
        "        net.train()\n",
        "        for input, target in train_loader:\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = net(input)\n",
        "            loss = loss_function(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss.append(loss.item())\n",
        "\n",
        "        mean_train_loss = np.mean(train_loss)\n",
        "        mean_psnr = mse_to_psnr(mean_train_loss)\n",
        "\n",
        "        if previous_loss - mean_train_loss < 0:\n",
        "          old_learning_rate = optimizer.param_groups[0]['lr']\n",
        "          optimizer.param_groups[0]['lr']\n",
        "          new_learning_rate = max(0.0001, learning_rate_factor * old_learning_rate - learning_rate_abs)\n",
        "          optimizer.param_groups[0]['lr'] = new_learning_rate\n",
        "\n",
        "          # Learning rate of the last layer should be 10 times lower\n",
        "          optimizer.param_groups[1]['lr'] = new_learning_rate/10\n",
        "\n",
        "\n",
        "        previous_loss = mean_train_loss\n",
        "          \n",
        "\n",
        "        # Update the lowest loss if necessary.\n",
        "        if mean_train_loss < lowest_loss[1]:\n",
        "            #print(f\"Epoch: {epoch: >3} Training Loss: {mean_train_loss:.6f} Mean PSNR: {mean_psnr:.2f} in {time.time() - begin_time:.2f}s #\")\n",
        "            lowest_loss = (epoch, mean_train_loss)\n",
        "            highest_psnr = mean_psnr\n",
        "          \n",
        "\n",
        "            #If the psnr is above some threshold save this new best network.\n",
        "            if highest_psnr > minimum_psnr_to_save:\n",
        "                torch.save(net, f'SuperResulutionNet_best_of_run-{computer_name}')\n",
        "\n",
        "        if epoch % 10 == 0 and print_output:\n",
        "          \n",
        "            print(\n",
        "              f\"Epoch: {epoch: >3} in {str(datetime.timedelta(seconds=int(time.time() - begin_time)))}, best epoch so far: Epoch: {lowest_loss[0]: >3} Training Loss: {lowest_loss[1]:.6f} Mean PSNR: {highest_psnr:.2f}, lr={ optimizer.param_groups[0]['lr']:.5f}\")\n",
        "\n",
        "\n",
        "        # If we didn't improve for some amount of epoch, lets stop.\n",
        "        if epoch > lowest_loss[0] + max_epochs_without_improvement:\n",
        "            if print_output:\n",
        "                print(f\"No improvement for the last {max_epochs_without_improvement} epochs, so stopping training...\")\n",
        "            break\n",
        "\n",
        "    net.eval()\n",
        "    if highest_psnr >= minimum_psnr_to_save:\n",
        "        network_name = f'SuperResulutionNet_r-{r}_psnr-{int(round(highest_psnr * 100))}__mse-{int(round(lowest_loss[1] * 10000))}-{computer_name}'\n",
        "        old_file = os.path.join(\".\", f'SuperResulutionNet_best_of_run-{computer_name}')\n",
        "        new_file = os.path.join(\".\", network_name)\n",
        "        if print_output:\n",
        "            print(f'Saving best epoch ({lowest_loss[0]}) with loss: {lowest_loss[1]} and psnr: {highest_psnr} as:')\n",
        "            print(network_name)\n",
        "        os.rename(old_file, new_file)\n",
        "    elif print_output:\n",
        "        print(\"Not high enough psnr to save the network...\")\n",
        "\n",
        "    return lowest_loss[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "1CywEXwxBLHo",
        "colab_type": "code",
        "outputId": "dcd358af-ac28-4f64-8ecc-0d97ee4a5f66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "use_gpu = torch.cuda.is_available()\n",
        "\n",
        "# Batch size.\n",
        "bs = 16\n",
        "\n",
        "# Upscale factor.\n",
        "r = 3\n",
        "\n",
        "# Amount of epochs.\n",
        "epochs = 20\n",
        "\n",
        "# Getting image data\n",
        "transform = transforms.Compose([transforms.ToTensor()])  # ,\n",
        "\n",
        "# Load the training data.\n",
        "training_set = SuperResolutionDataset(\"train_data/Set91\", r, use_gpu=use_gpu)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(training_set, batch_size=bs, shuffle=True)\n",
        "\n",
        "# Initialize the network.\n",
        "net = SuperResolutionNet(r, activation=nn.ReLU())\n",
        "\n",
        "train(net, use_gpu, train_loader, r)"
      ],
      "execution_count": 291,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  50 in 0:00:10, best epoch so far: Epoch:  50 Training Loss: 0.006712 Mean PSNR: 21.73, lr=0.01000\n",
            "Epoch:  60 in 0:00:12, best epoch so far: Epoch:  60 Training Loss: 0.006247 Mean PSNR: 22.04, lr=0.01000\n",
            "Epoch:  70 in 0:00:14, best epoch so far: Epoch:  70 Training Loss: 0.005872 Mean PSNR: 22.31, lr=0.01000\n",
            "Epoch:  80 in 0:00:16, best epoch so far: Epoch:  80 Training Loss: 0.005582 Mean PSNR: 22.53, lr=0.01000\n",
            "Epoch:  90 in 0:00:18, best epoch so far: Epoch:  90 Training Loss: 0.005317 Mean PSNR: 22.74, lr=0.01000\n",
            "Epoch: 100 in 0:00:20, best epoch so far: Epoch: 100 Training Loss: 0.005090 Mean PSNR: 22.93, lr=0.01000\n",
            "Epoch: 110 in 0:00:22, best epoch so far: Epoch: 110 Training Loss: 0.004880 Mean PSNR: 23.12, lr=0.00999\n",
            "Epoch: 120 in 0:00:24, best epoch so far: Epoch: 120 Training Loss: 0.004698 Mean PSNR: 23.28, lr=0.00999\n",
            "Epoch: 130 in 0:00:26, best epoch so far: Epoch: 130 Training Loss: 0.004534 Mean PSNR: 23.43, lr=0.00998\n",
            "Epoch: 140 in 0:00:28, best epoch so far: Epoch: 139 Training Loss: 0.004413 Mean PSNR: 23.55, lr=0.00996\n",
            "Epoch: 150 in 0:00:30, best epoch so far: Epoch: 150 Training Loss: 0.004277 Mean PSNR: 23.69, lr=0.00994\n",
            "Epoch: 160 in 0:00:33, best epoch so far: Epoch: 159 Training Loss: 0.004171 Mean PSNR: 23.80, lr=0.00990\n",
            "Epoch: 170 in 0:00:35, best epoch so far: Epoch: 170 Training Loss: 0.004076 Mean PSNR: 23.90, lr=0.00990\n",
            "Epoch: 180 in 0:00:37, best epoch so far: Epoch: 180 Training Loss: 0.003988 Mean PSNR: 23.99, lr=0.00988\n",
            "Epoch: 190 in 0:00:39, best epoch so far: Epoch: 190 Training Loss: 0.003904 Mean PSNR: 24.08, lr=0.00985\n",
            "Epoch: 200 in 0:00:41, best epoch so far: Epoch: 200 Training Loss: 0.003828 Mean PSNR: 24.17, lr=0.00984\n",
            "Epoch: 210 in 0:00:43, best epoch so far: Epoch: 210 Training Loss: 0.003763 Mean PSNR: 24.24, lr=0.00983\n",
            "Epoch: 220 in 0:00:45, best epoch so far: Epoch: 220 Training Loss: 0.003704 Mean PSNR: 24.31, lr=0.00981\n",
            "Epoch: 230 in 0:00:47, best epoch so far: Epoch: 230 Training Loss: 0.003645 Mean PSNR: 24.38, lr=0.00976\n",
            "Epoch: 240 in 0:00:49, best epoch so far: Epoch: 240 Training Loss: 0.003597 Mean PSNR: 24.44, lr=0.00975\n",
            "Epoch: 250 in 0:00:51, best epoch so far: Epoch: 250 Training Loss: 0.003544 Mean PSNR: 24.50, lr=0.00974\n",
            "Epoch: 260 in 0:00:53, best epoch so far: Epoch: 260 Training Loss: 0.003500 Mean PSNR: 24.56, lr=0.00973\n",
            "Epoch: 270 in 0:00:55, best epoch so far: Epoch: 270 Training Loss: 0.003457 Mean PSNR: 24.61, lr=0.00972\n",
            "Epoch: 280 in 0:00:57, best epoch so far: Epoch: 279 Training Loss: 0.003416 Mean PSNR: 24.66, lr=0.00970\n",
            "Epoch: 290 in 0:00:59, best epoch so far: Epoch: 290 Training Loss: 0.003379 Mean PSNR: 24.71, lr=0.00968\n",
            "Epoch: 300 in 0:01:01, best epoch so far: Epoch: 300 Training Loss: 0.003348 Mean PSNR: 24.75, lr=0.00965\n",
            "Epoch: 310 in 0:01:03, best epoch so far: Epoch: 310 Training Loss: 0.003304 Mean PSNR: 24.81, lr=0.00962\n",
            "Epoch: 320 in 0:01:05, best epoch so far: Epoch: 320 Training Loss: 0.003276 Mean PSNR: 24.85, lr=0.00959\n",
            "Epoch: 330 in 0:01:07, best epoch so far: Epoch: 330 Training Loss: 0.003248 Mean PSNR: 24.88, lr=0.00954\n",
            "Epoch: 340 in 0:01:10, best epoch so far: Epoch: 340 Training Loss: 0.003213 Mean PSNR: 24.93, lr=0.00951\n",
            "Epoch: 350 in 0:01:12, best epoch so far: Epoch: 350 Training Loss: 0.003184 Mean PSNR: 24.97, lr=0.00948\n",
            "Epoch: 360 in 0:01:14, best epoch so far: Epoch: 359 Training Loss: 0.003162 Mean PSNR: 25.00, lr=0.00945\n",
            "Epoch: 370 in 0:01:16, best epoch so far: Epoch: 369 Training Loss: 0.003132 Mean PSNR: 25.04, lr=0.00940\n",
            "Epoch: 380 in 0:01:18, best epoch so far: Epoch: 380 Training Loss: 0.003105 Mean PSNR: 25.08, lr=0.00937\n",
            "Epoch: 390 in 0:01:20, best epoch so far: Epoch: 390 Training Loss: 0.003086 Mean PSNR: 25.11, lr=0.00934\n",
            "Epoch: 400 in 0:01:22, best epoch so far: Epoch: 398 Training Loss: 0.003068 Mean PSNR: 25.13, lr=0.00930\n",
            "Epoch: 410 in 0:01:24, best epoch so far: Epoch: 410 Training Loss: 0.003050 Mean PSNR: 25.16, lr=0.00925\n",
            "Epoch: 420 in 0:01:26, best epoch so far: Epoch: 420 Training Loss: 0.003026 Mean PSNR: 25.19, lr=0.00922\n",
            "Epoch: 430 in 0:01:28, best epoch so far: Epoch: 430 Training Loss: 0.003010 Mean PSNR: 25.21, lr=0.00919\n",
            "Epoch: 440 in 0:01:30, best epoch so far: Epoch: 438 Training Loss: 0.002992 Mean PSNR: 25.24, lr=0.00914\n",
            "Epoch: 450 in 0:01:32, best epoch so far: Epoch: 450 Training Loss: 0.002972 Mean PSNR: 25.27, lr=0.00911\n",
            "Epoch: 460 in 0:01:34, best epoch so far: Epoch: 460 Training Loss: 0.002951 Mean PSNR: 25.30, lr=0.00906\n",
            "Epoch: 470 in 0:01:36, best epoch so far: Epoch: 470 Training Loss: 0.002939 Mean PSNR: 25.32, lr=0.00904\n",
            "Epoch: 480 in 0:01:38, best epoch so far: Epoch: 479 Training Loss: 0.002926 Mean PSNR: 25.34, lr=0.00900\n",
            "Epoch: 490 in 0:01:40, best epoch so far: Epoch: 488 Training Loss: 0.002914 Mean PSNR: 25.36, lr=0.00897\n",
            "Epoch: 500 in 0:01:42, best epoch so far: Epoch: 496 Training Loss: 0.002898 Mean PSNR: 25.38, lr=0.00892\n",
            "Epoch: 510 in 0:01:44, best epoch so far: Epoch: 510 Training Loss: 0.002887 Mean PSNR: 25.40, lr=0.00889\n",
            "Epoch: 520 in 0:01:46, best epoch so far: Epoch: 517 Training Loss: 0.002870 Mean PSNR: 25.42, lr=0.00884\n",
            "Epoch: 530 in 0:01:48, best epoch so far: Epoch: 530 Training Loss: 0.002855 Mean PSNR: 25.44, lr=0.00880\n",
            "Epoch: 540 in 0:01:50, best epoch so far: Epoch: 537 Training Loss: 0.002846 Mean PSNR: 25.46, lr=0.00875\n",
            "Epoch: 550 in 0:01:52, best epoch so far: Epoch: 550 Training Loss: 0.002830 Mean PSNR: 25.48, lr=0.00871\n",
            "Epoch: 560 in 0:01:54, best epoch so far: Epoch: 558 Training Loss: 0.002825 Mean PSNR: 25.49, lr=0.00865\n",
            "Epoch: 570 in 0:01:56, best epoch so far: Epoch: 568 Training Loss: 0.002814 Mean PSNR: 25.51, lr=0.00861\n",
            "Epoch: 580 in 0:01:58, best epoch so far: Epoch: 578 Training Loss: 0.002800 Mean PSNR: 25.53, lr=0.00856\n",
            "Epoch: 590 in 0:02:00, best epoch so far: Epoch: 590 Training Loss: 0.002790 Mean PSNR: 25.54, lr=0.00854\n",
            "Epoch: 600 in 0:02:02, best epoch so far: Epoch: 598 Training Loss: 0.002785 Mean PSNR: 25.55, lr=0.00849\n",
            "Epoch: 610 in 0:02:04, best epoch so far: Epoch: 605 Training Loss: 0.002775 Mean PSNR: 25.57, lr=0.00843\n",
            "Epoch: 620 in 0:02:06, best epoch so far: Epoch: 620 Training Loss: 0.002767 Mean PSNR: 25.58, lr=0.00839\n",
            "Epoch: 630 in 0:02:09, best epoch so far: Epoch: 629 Training Loss: 0.002755 Mean PSNR: 25.60, lr=0.00835\n",
            "Epoch: 640 in 0:02:11, best epoch so far: Epoch: 638 Training Loss: 0.002747 Mean PSNR: 25.61, lr=0.00831\n",
            "Epoch: 650 in 0:02:13, best epoch so far: Epoch: 648 Training Loss: 0.002740 Mean PSNR: 25.62, lr=0.00825\n",
            "Epoch: 660 in 0:02:15, best epoch so far: Epoch: 655 Training Loss: 0.002734 Mean PSNR: 25.63, lr=0.00820\n",
            "Epoch: 670 in 0:02:17, best epoch so far: Epoch: 666 Training Loss: 0.002728 Mean PSNR: 25.64, lr=0.00816\n",
            "Epoch: 680 in 0:02:19, best epoch so far: Epoch: 679 Training Loss: 0.002714 Mean PSNR: 25.66, lr=0.00812\n",
            "Epoch: 690 in 0:02:21, best epoch so far: Epoch: 690 Training Loss: 0.002709 Mean PSNR: 25.67, lr=0.00808\n",
            "Epoch: 700 in 0:02:23, best epoch so far: Epoch: 698 Training Loss: 0.002704 Mean PSNR: 25.68, lr=0.00804\n",
            "Epoch: 710 in 0:02:25, best epoch so far: Epoch: 710 Training Loss: 0.002694 Mean PSNR: 25.70, lr=0.00800\n",
            "Epoch: 720 in 0:02:27, best epoch so far: Epoch: 719 Training Loss: 0.002689 Mean PSNR: 25.70, lr=0.00796\n",
            "Epoch: 730 in 0:02:29, best epoch so far: Epoch: 719 Training Loss: 0.002689 Mean PSNR: 25.70, lr=0.00791\n",
            "Epoch: 740 in 0:02:31, best epoch so far: Epoch: 740 Training Loss: 0.002676 Mean PSNR: 25.73, lr=0.00788\n",
            "Epoch: 750 in 0:02:33, best epoch so far: Epoch: 747 Training Loss: 0.002672 Mean PSNR: 25.73, lr=0.00782\n",
            "Epoch: 760 in 0:02:35, best epoch so far: Epoch: 755 Training Loss: 0.002667 Mean PSNR: 25.74, lr=0.00778\n",
            "Epoch: 770 in 0:02:37, best epoch so far: Epoch: 762 Training Loss: 0.002663 Mean PSNR: 25.75, lr=0.00773\n",
            "Epoch: 780 in 0:02:39, best epoch so far: Epoch: 777 Training Loss: 0.002657 Mean PSNR: 25.76, lr=0.00769\n",
            "Epoch: 790 in 0:02:41, best epoch so far: Epoch: 785 Training Loss: 0.002653 Mean PSNR: 25.76, lr=0.00765\n",
            "Epoch: 800 in 0:02:43, best epoch so far: Epoch: 798 Training Loss: 0.002644 Mean PSNR: 25.78, lr=0.00760\n",
            "Epoch: 810 in 0:02:45, best epoch so far: Epoch: 808 Training Loss: 0.002643 Mean PSNR: 25.78, lr=0.00757\n",
            "Epoch: 820 in 0:02:47, best epoch so far: Epoch: 817 Training Loss: 0.002641 Mean PSNR: 25.78, lr=0.00751\n",
            "Epoch: 830 in 0:02:49, best epoch so far: Epoch: 826 Training Loss: 0.002634 Mean PSNR: 25.79, lr=0.00746\n",
            "Epoch: 840 in 0:02:51, best epoch so far: Epoch: 838 Training Loss: 0.002631 Mean PSNR: 25.80, lr=0.00742\n",
            "Epoch: 850 in 0:02:53, best epoch so far: Epoch: 850 Training Loss: 0.002626 Mean PSNR: 25.81, lr=0.00737\n",
            "Epoch: 860 in 0:02:55, best epoch so far: Epoch: 851 Training Loss: 0.002621 Mean PSNR: 25.82, lr=0.00732\n",
            "Epoch: 870 in 0:02:57, best epoch so far: Epoch: 866 Training Loss: 0.002618 Mean PSNR: 25.82, lr=0.00728\n",
            "Epoch: 880 in 0:02:59, best epoch so far: Epoch: 880 Training Loss: 0.002616 Mean PSNR: 25.82, lr=0.00724\n",
            "Epoch: 890 in 0:03:01, best epoch so far: Epoch: 882 Training Loss: 0.002609 Mean PSNR: 25.84, lr=0.00718\n",
            "Epoch: 900 in 0:03:04, best epoch so far: Epoch: 899 Training Loss: 0.002603 Mean PSNR: 25.85, lr=0.00713\n",
            "Epoch: 910 in 0:03:06, best epoch so far: Epoch: 906 Training Loss: 0.002601 Mean PSNR: 25.85, lr=0.00708\n",
            "Epoch: 920 in 0:03:08, best epoch so far: Epoch: 919 Training Loss: 0.002597 Mean PSNR: 25.86, lr=0.00702\n",
            "Epoch: 930 in 0:03:10, best epoch so far: Epoch: 919 Training Loss: 0.002597 Mean PSNR: 25.86, lr=0.00696\n",
            "Epoch: 940 in 0:03:12, best epoch so far: Epoch: 940 Training Loss: 0.002591 Mean PSNR: 25.87, lr=0.00692\n",
            "Epoch: 950 in 0:03:14, best epoch so far: Epoch: 946 Training Loss: 0.002588 Mean PSNR: 25.87, lr=0.00687\n",
            "Epoch: 960 in 0:03:16, best epoch so far: Epoch: 958 Training Loss: 0.002585 Mean PSNR: 25.88, lr=0.00683\n",
            "Epoch: 970 in 0:03:18, best epoch so far: Epoch: 968 Training Loss: 0.002579 Mean PSNR: 25.89, lr=0.00677\n",
            "Epoch: 980 in 0:03:20, best epoch so far: Epoch: 975 Training Loss: 0.002575 Mean PSNR: 25.89, lr=0.00674\n",
            "Epoch: 990 in 0:03:22, best epoch so far: Epoch: 981 Training Loss: 0.002574 Mean PSNR: 25.89, lr=0.00668\n",
            "Epoch: 1000 in 0:03:24, best epoch so far: Epoch: 992 Training Loss: 0.002571 Mean PSNR: 25.90, lr=0.00664\n",
            "Epoch: 1010 in 0:03:26, best epoch so far: Epoch: 1006 Training Loss: 0.002566 Mean PSNR: 25.91, lr=0.00661\n",
            "Epoch: 1020 in 0:03:28, best epoch so far: Epoch: 1012 Training Loss: 0.002565 Mean PSNR: 25.91, lr=0.00655\n",
            "Epoch: 1030 in 0:03:30, best epoch so far: Epoch: 1022 Training Loss: 0.002564 Mean PSNR: 25.91, lr=0.00649\n",
            "Epoch: 1040 in 0:03:32, best epoch so far: Epoch: 1037 Training Loss: 0.002560 Mean PSNR: 25.92, lr=0.00644\n",
            "Epoch: 1050 in 0:03:34, best epoch so far: Epoch: 1042 Training Loss: 0.002559 Mean PSNR: 25.92, lr=0.00639\n",
            "Epoch: 1060 in 0:03:36, best epoch so far: Epoch: 1054 Training Loss: 0.002555 Mean PSNR: 25.93, lr=0.00634\n",
            "Epoch: 1070 in 0:03:38, best epoch so far: Epoch: 1054 Training Loss: 0.002555 Mean PSNR: 25.93, lr=0.00630\n",
            "Epoch: 1080 in 0:03:40, best epoch so far: Epoch: 1080 Training Loss: 0.002551 Mean PSNR: 25.93, lr=0.00625\n",
            "Epoch: 1090 in 0:03:42, best epoch so far: Epoch: 1089 Training Loss: 0.002547 Mean PSNR: 25.94, lr=0.00619\n",
            "Epoch: 1100 in 0:03:44, best epoch so far: Epoch: 1093 Training Loss: 0.002544 Mean PSNR: 25.94, lr=0.00615\n",
            "Epoch: 1110 in 0:03:46, best epoch so far: Epoch: 1093 Training Loss: 0.002544 Mean PSNR: 25.94, lr=0.00612\n",
            "Epoch: 1120 in 0:03:48, best epoch so far: Epoch: 1117 Training Loss: 0.002541 Mean PSNR: 25.95, lr=0.00608\n",
            "Epoch: 1130 in 0:03:50, best epoch so far: Epoch: 1125 Training Loss: 0.002538 Mean PSNR: 25.96, lr=0.00603\n",
            "Epoch: 1140 in 0:03:52, best epoch so far: Epoch: 1140 Training Loss: 0.002536 Mean PSNR: 25.96, lr=0.00599\n",
            "Epoch: 1150 in 0:03:54, best epoch so far: Epoch: 1146 Training Loss: 0.002533 Mean PSNR: 25.96, lr=0.00594\n",
            "Epoch: 1160 in 0:03:56, best epoch so far: Epoch: 1157 Training Loss: 0.002531 Mean PSNR: 25.97, lr=0.00588\n",
            "Epoch: 1170 in 0:03:58, best epoch so far: Epoch: 1166 Training Loss: 0.002531 Mean PSNR: 25.97, lr=0.00582\n",
            "Epoch: 1180 in 0:04:00, best epoch so far: Epoch: 1180 Training Loss: 0.002525 Mean PSNR: 25.98, lr=0.00578\n",
            "Epoch: 1190 in 0:04:02, best epoch so far: Epoch: 1187 Training Loss: 0.002523 Mean PSNR: 25.98, lr=0.00572\n",
            "Epoch: 1200 in 0:04:04, best epoch so far: Epoch: 1191 Training Loss: 0.002523 Mean PSNR: 25.98, lr=0.00567\n",
            "Epoch: 1210 in 0:04:06, best epoch so far: Epoch: 1203 Training Loss: 0.002523 Mean PSNR: 25.98, lr=0.00562\n",
            "Epoch: 1220 in 0:04:08, best epoch so far: Epoch: 1220 Training Loss: 0.002520 Mean PSNR: 25.99, lr=0.00558\n",
            "Epoch: 1230 in 0:04:10, best epoch so far: Epoch: 1226 Training Loss: 0.002519 Mean PSNR: 25.99, lr=0.00553\n",
            "Epoch: 1240 in 0:04:12, best epoch so far: Epoch: 1237 Training Loss: 0.002515 Mean PSNR: 26.00, lr=0.00548\n",
            "Epoch: 1250 in 0:04:14, best epoch so far: Epoch: 1237 Training Loss: 0.002515 Mean PSNR: 26.00, lr=0.00544\n",
            "Epoch: 1260 in 0:04:16, best epoch so far: Epoch: 1255 Training Loss: 0.002514 Mean PSNR: 26.00, lr=0.00538\n",
            "Epoch: 1270 in 0:04:18, best epoch so far: Epoch: 1265 Training Loss: 0.002512 Mean PSNR: 26.00, lr=0.00535\n",
            "Epoch: 1280 in 0:04:20, best epoch so far: Epoch: 1279 Training Loss: 0.002511 Mean PSNR: 26.00, lr=0.00530\n",
            "Epoch: 1290 in 0:04:22, best epoch so far: Epoch: 1287 Training Loss: 0.002508 Mean PSNR: 26.01, lr=0.00524\n",
            "Epoch: 1300 in 0:04:25, best epoch so far: Epoch: 1296 Training Loss: 0.002507 Mean PSNR: 26.01, lr=0.00519\n",
            "Epoch: 1310 in 0:04:27, best epoch so far: Epoch: 1296 Training Loss: 0.002507 Mean PSNR: 26.01, lr=0.00513\n",
            "Epoch: 1320 in 0:04:29, best epoch so far: Epoch: 1320 Training Loss: 0.002501 Mean PSNR: 26.02, lr=0.00510\n",
            "Epoch: 1330 in 0:04:31, best epoch so far: Epoch: 1320 Training Loss: 0.002501 Mean PSNR: 26.02, lr=0.00504\n",
            "Epoch: 1340 in 0:04:33, best epoch so far: Epoch: 1332 Training Loss: 0.002500 Mean PSNR: 26.02, lr=0.00498\n",
            "Epoch: 1350 in 0:04:35, best epoch so far: Epoch: 1347 Training Loss: 0.002500 Mean PSNR: 26.02, lr=0.00493\n",
            "Epoch: 1360 in 0:04:37, best epoch so far: Epoch: 1355 Training Loss: 0.002497 Mean PSNR: 26.03, lr=0.00488\n",
            "Epoch: 1370 in 0:04:39, best epoch so far: Epoch: 1370 Training Loss: 0.002495 Mean PSNR: 26.03, lr=0.00485\n",
            "Epoch: 1380 in 0:04:41, best epoch so far: Epoch: 1380 Training Loss: 0.002493 Mean PSNR: 26.03, lr=0.00480\n",
            "Epoch: 1390 in 0:04:43, best epoch so far: Epoch: 1390 Training Loss: 0.002490 Mean PSNR: 26.04, lr=0.00475\n",
            "Epoch: 1400 in 0:04:45, best epoch so far: Epoch: 1398 Training Loss: 0.002489 Mean PSNR: 26.04, lr=0.00470\n",
            "Epoch: 1410 in 0:04:47, best epoch so far: Epoch: 1398 Training Loss: 0.002489 Mean PSNR: 26.04, lr=0.00466\n",
            "Epoch: 1420 in 0:04:49, best epoch so far: Epoch: 1417 Training Loss: 0.002486 Mean PSNR: 26.04, lr=0.00461\n",
            "Epoch: 1430 in 0:04:51, best epoch so far: Epoch: 1417 Training Loss: 0.002486 Mean PSNR: 26.04, lr=0.00456\n",
            "Epoch: 1440 in 0:04:54, best epoch so far: Epoch: 1417 Training Loss: 0.002486 Mean PSNR: 26.04, lr=0.00453\n",
            "Epoch: 1450 in 0:04:56, best epoch so far: Epoch: 1417 Training Loss: 0.002486 Mean PSNR: 26.04, lr=0.00447\n",
            "Epoch: 1460 in 0:04:57, best epoch so far: Epoch: 1417 Training Loss: 0.002486 Mean PSNR: 26.04, lr=0.00444\n",
            "Epoch: 1470 in 0:04:59, best epoch so far: Epoch: 1468 Training Loss: 0.002482 Mean PSNR: 26.05, lr=0.00440\n",
            "Epoch: 1480 in 0:05:02, best epoch so far: Epoch: 1472 Training Loss: 0.002481 Mean PSNR: 26.05, lr=0.00437\n",
            "Epoch: 1490 in 0:05:04, best epoch so far: Epoch: 1490 Training Loss: 0.002481 Mean PSNR: 26.05, lr=0.00433\n",
            "Epoch: 1500 in 0:05:06, best epoch so far: Epoch: 1490 Training Loss: 0.002481 Mean PSNR: 26.05, lr=0.00427\n",
            "Epoch: 1510 in 0:05:08, best epoch so far: Epoch: 1506 Training Loss: 0.002477 Mean PSNR: 26.06, lr=0.00423\n",
            "Epoch: 1520 in 0:05:10, best epoch so far: Epoch: 1518 Training Loss: 0.002476 Mean PSNR: 26.06, lr=0.00418\n",
            "Epoch: 1530 in 0:05:12, best epoch so far: Epoch: 1518 Training Loss: 0.002476 Mean PSNR: 26.06, lr=0.00413\n",
            "Epoch: 1540 in 0:05:14, best epoch so far: Epoch: 1537 Training Loss: 0.002473 Mean PSNR: 26.07, lr=0.00408\n",
            "Epoch: 1550 in 0:05:16, best epoch so far: Epoch: 1537 Training Loss: 0.002473 Mean PSNR: 26.07, lr=0.00403\n",
            "Epoch: 1560 in 0:05:18, best epoch so far: Epoch: 1537 Training Loss: 0.002473 Mean PSNR: 26.07, lr=0.00398\n",
            "Epoch: 1570 in 0:05:20, best epoch so far: Epoch: 1537 Training Loss: 0.002473 Mean PSNR: 26.07, lr=0.00394\n",
            "Epoch: 1580 in 0:05:22, best epoch so far: Epoch: 1574 Training Loss: 0.002469 Mean PSNR: 26.07, lr=0.00388\n",
            "Epoch: 1590 in 0:05:24, best epoch so far: Epoch: 1574 Training Loss: 0.002469 Mean PSNR: 26.07, lr=0.00384\n",
            "Epoch: 1600 in 0:05:26, best epoch so far: Epoch: 1574 Training Loss: 0.002469 Mean PSNR: 26.07, lr=0.00381\n",
            "Epoch: 1610 in 0:05:28, best epoch so far: Epoch: 1605 Training Loss: 0.002466 Mean PSNR: 26.08, lr=0.00378\n",
            "Epoch: 1620 in 0:05:30, best epoch so far: Epoch: 1605 Training Loss: 0.002466 Mean PSNR: 26.08, lr=0.00373\n",
            "Epoch: 1630 in 0:05:32, best epoch so far: Epoch: 1630 Training Loss: 0.002466 Mean PSNR: 26.08, lr=0.00369\n",
            "Epoch: 1640 in 0:05:35, best epoch so far: Epoch: 1638 Training Loss: 0.002463 Mean PSNR: 26.08, lr=0.00363\n",
            "Epoch: 1650 in 0:05:37, best epoch so far: Epoch: 1638 Training Loss: 0.002463 Mean PSNR: 26.08, lr=0.00359\n",
            "Epoch: 1660 in 0:05:39, best epoch so far: Epoch: 1659 Training Loss: 0.002463 Mean PSNR: 26.09, lr=0.00355\n",
            "Epoch: 1670 in 0:05:41, best epoch so far: Epoch: 1663 Training Loss: 0.002461 Mean PSNR: 26.09, lr=0.00351\n",
            "Epoch: 1680 in 0:05:43, best epoch so far: Epoch: 1663 Training Loss: 0.002461 Mean PSNR: 26.09, lr=0.00344\n",
            "Epoch: 1690 in 0:05:45, best epoch so far: Epoch: 1685 Training Loss: 0.002461 Mean PSNR: 26.09, lr=0.00339\n",
            "Epoch: 1700 in 0:05:47, best epoch so far: Epoch: 1694 Training Loss: 0.002460 Mean PSNR: 26.09, lr=0.00333\n",
            "Epoch: 1710 in 0:05:49, best epoch so far: Epoch: 1706 Training Loss: 0.002458 Mean PSNR: 26.09, lr=0.00328\n",
            "Epoch: 1720 in 0:05:51, best epoch so far: Epoch: 1706 Training Loss: 0.002458 Mean PSNR: 26.09, lr=0.00324\n",
            "Epoch: 1730 in 0:05:53, best epoch so far: Epoch: 1706 Training Loss: 0.002458 Mean PSNR: 26.09, lr=0.00319\n",
            "Epoch: 1740 in 0:05:55, best epoch so far: Epoch: 1706 Training Loss: 0.002458 Mean PSNR: 26.09, lr=0.00314\n",
            "Epoch: 1750 in 0:05:57, best epoch so far: Epoch: 1744 Training Loss: 0.002456 Mean PSNR: 26.10, lr=0.00308\n",
            "Epoch: 1760 in 0:05:59, best epoch so far: Epoch: 1744 Training Loss: 0.002456 Mean PSNR: 26.10, lr=0.00304\n",
            "Epoch: 1770 in 0:06:01, best epoch so far: Epoch: 1767 Training Loss: 0.002455 Mean PSNR: 26.10, lr=0.00299\n",
            "Epoch: 1780 in 0:06:03, best epoch so far: Epoch: 1767 Training Loss: 0.002455 Mean PSNR: 26.10, lr=0.00292\n",
            "Epoch: 1790 in 0:06:05, best epoch so far: Epoch: 1789 Training Loss: 0.002454 Mean PSNR: 26.10, lr=0.00287\n",
            "Epoch: 1800 in 0:06:07, best epoch so far: Epoch: 1797 Training Loss: 0.002453 Mean PSNR: 26.10, lr=0.00281\n",
            "Epoch: 1810 in 0:06:09, best epoch so far: Epoch: 1806 Training Loss: 0.002452 Mean PSNR: 26.11, lr=0.00276\n",
            "Epoch: 1820 in 0:06:11, best epoch so far: Epoch: 1815 Training Loss: 0.002451 Mean PSNR: 26.11, lr=0.00272\n",
            "Epoch: 1830 in 0:06:13, best epoch so far: Epoch: 1815 Training Loss: 0.002451 Mean PSNR: 26.11, lr=0.00266\n",
            "Epoch: 1840 in 0:06:15, best epoch so far: Epoch: 1815 Training Loss: 0.002451 Mean PSNR: 26.11, lr=0.00261\n",
            "Epoch: 1850 in 0:06:17, best epoch so far: Epoch: 1815 Training Loss: 0.002451 Mean PSNR: 26.11, lr=0.00256\n",
            "Epoch: 1860 in 0:06:19, best epoch so far: Epoch: 1852 Training Loss: 0.002450 Mean PSNR: 26.11, lr=0.00249\n",
            "Epoch: 1870 in 0:06:21, best epoch so far: Epoch: 1852 Training Loss: 0.002450 Mean PSNR: 26.11, lr=0.00245\n",
            "Epoch: 1880 in 0:06:23, best epoch so far: Epoch: 1880 Training Loss: 0.002448 Mean PSNR: 26.11, lr=0.00242\n",
            "Epoch: 1890 in 0:06:25, best epoch so far: Epoch: 1890 Training Loss: 0.002445 Mean PSNR: 26.12, lr=0.00238\n",
            "Epoch: 1900 in 0:06:27, best epoch so far: Epoch: 1890 Training Loss: 0.002445 Mean PSNR: 26.12, lr=0.00233\n",
            "Epoch: 1910 in 0:06:29, best epoch so far: Epoch: 1890 Training Loss: 0.002445 Mean PSNR: 26.12, lr=0.00226\n",
            "Epoch: 1920 in 0:06:31, best epoch so far: Epoch: 1890 Training Loss: 0.002445 Mean PSNR: 26.12, lr=0.00223\n",
            "Epoch: 1930 in 0:06:33, best epoch so far: Epoch: 1890 Training Loss: 0.002445 Mean PSNR: 26.12, lr=0.00218\n",
            "Epoch: 1940 in 0:06:35, best epoch so far: Epoch: 1936 Training Loss: 0.002445 Mean PSNR: 26.12, lr=0.00212\n",
            "Epoch: 1950 in 0:06:37, best epoch so far: Epoch: 1936 Training Loss: 0.002445 Mean PSNR: 26.12, lr=0.00207\n",
            "Epoch: 1960 in 0:06:39, best epoch so far: Epoch: 1954 Training Loss: 0.002444 Mean PSNR: 26.12, lr=0.00202\n",
            "Epoch: 1970 in 0:06:41, best epoch so far: Epoch: 1966 Training Loss: 0.002443 Mean PSNR: 26.12, lr=0.00196\n",
            "Epoch: 1980 in 0:06:43, best epoch so far: Epoch: 1966 Training Loss: 0.002443 Mean PSNR: 26.12, lr=0.00191\n",
            "Epoch: 1990 in 0:06:45, best epoch so far: Epoch: 1985 Training Loss: 0.002442 Mean PSNR: 26.12, lr=0.00186\n",
            "Epoch: 2000 in 0:06:47, best epoch so far: Epoch: 1985 Training Loss: 0.002442 Mean PSNR: 26.12, lr=0.00180\n",
            "Epoch: 2010 in 0:06:50, best epoch so far: Epoch: 1985 Training Loss: 0.002442 Mean PSNR: 26.12, lr=0.00176\n",
            "Epoch: 2020 in 0:06:52, best epoch so far: Epoch: 1985 Training Loss: 0.002442 Mean PSNR: 26.12, lr=0.00171\n",
            "Epoch: 2030 in 0:06:54, best epoch so far: Epoch: 1985 Training Loss: 0.002442 Mean PSNR: 26.12, lr=0.00166\n",
            "Epoch: 2040 in 0:06:56, best epoch so far: Epoch: 2039 Training Loss: 0.002442 Mean PSNR: 26.12, lr=0.00161\n",
            "Epoch: 2050 in 0:06:58, best epoch so far: Epoch: 2047 Training Loss: 0.002440 Mean PSNR: 26.13, lr=0.00155\n",
            "Epoch: 2060 in 0:07:00, best epoch so far: Epoch: 2057 Training Loss: 0.002439 Mean PSNR: 26.13, lr=0.00149\n",
            "Epoch: 2070 in 0:07:02, best epoch so far: Epoch: 2057 Training Loss: 0.002439 Mean PSNR: 26.13, lr=0.00144\n",
            "Epoch: 2080 in 0:07:04, best epoch so far: Epoch: 2057 Training Loss: 0.002439 Mean PSNR: 26.13, lr=0.00136\n",
            "Epoch: 2090 in 0:07:06, best epoch so far: Epoch: 2057 Training Loss: 0.002439 Mean PSNR: 26.13, lr=0.00131\n",
            "Epoch: 2100 in 0:07:08, best epoch so far: Epoch: 2057 Training Loss: 0.002439 Mean PSNR: 26.13, lr=0.00127\n",
            "Epoch: 2110 in 0:07:10, best epoch so far: Epoch: 2105 Training Loss: 0.002436 Mean PSNR: 26.13, lr=0.00121\n",
            "Epoch: 2120 in 0:07:12, best epoch so far: Epoch: 2105 Training Loss: 0.002436 Mean PSNR: 26.13, lr=0.00115\n",
            "Epoch: 2130 in 0:07:14, best epoch so far: Epoch: 2105 Training Loss: 0.002436 Mean PSNR: 26.13, lr=0.00110\n",
            "Epoch: 2140 in 0:07:16, best epoch so far: Epoch: 2105 Training Loss: 0.002436 Mean PSNR: 26.13, lr=0.00104\n",
            "Epoch: 2150 in 0:07:18, best epoch so far: Epoch: 2105 Training Loss: 0.002436 Mean PSNR: 26.13, lr=0.00099\n",
            "Epoch: 2160 in 0:07:20, best epoch so far: Epoch: 2105 Training Loss: 0.002436 Mean PSNR: 26.13, lr=0.00095\n",
            "Epoch: 2170 in 0:07:22, best epoch so far: Epoch: 2161 Training Loss: 0.002436 Mean PSNR: 26.13, lr=0.00090\n",
            "Epoch: 2180 in 0:07:24, best epoch so far: Epoch: 2161 Training Loss: 0.002436 Mean PSNR: 26.13, lr=0.00085\n",
            "Epoch: 2190 in 0:07:26, best epoch so far: Epoch: 2161 Training Loss: 0.002436 Mean PSNR: 26.13, lr=0.00082\n",
            "Epoch: 2200 in 0:07:28, best epoch so far: Epoch: 2161 Training Loss: 0.002436 Mean PSNR: 26.13, lr=0.00078\n",
            "Epoch: 2210 in 0:07:30, best epoch so far: Epoch: 2209 Training Loss: 0.002435 Mean PSNR: 26.13, lr=0.00073\n",
            "Epoch: 2220 in 0:07:32, best epoch so far: Epoch: 2209 Training Loss: 0.002435 Mean PSNR: 26.13, lr=0.00068\n",
            "Epoch: 2230 in 0:07:34, best epoch so far: Epoch: 2221 Training Loss: 0.002435 Mean PSNR: 26.13, lr=0.00064\n",
            "Epoch: 2240 in 0:07:36, best epoch so far: Epoch: 2221 Training Loss: 0.002435 Mean PSNR: 26.13, lr=0.00059\n",
            "Epoch: 2250 in 0:07:38, best epoch so far: Epoch: 2221 Training Loss: 0.002435 Mean PSNR: 26.13, lr=0.00054\n",
            "Epoch: 2260 in 0:07:40, best epoch so far: Epoch: 2260 Training Loss: 0.002434 Mean PSNR: 26.14, lr=0.00050\n",
            "Epoch: 2270 in 0:07:42, best epoch so far: Epoch: 2260 Training Loss: 0.002434 Mean PSNR: 26.14, lr=0.00045\n",
            "Epoch: 2280 in 0:07:44, best epoch so far: Epoch: 2260 Training Loss: 0.002434 Mean PSNR: 26.14, lr=0.00040\n",
            "Epoch: 2290 in 0:07:46, best epoch so far: Epoch: 2285 Training Loss: 0.002434 Mean PSNR: 26.14, lr=0.00035\n",
            "Epoch: 2300 in 0:07:48, best epoch so far: Epoch: 2285 Training Loss: 0.002434 Mean PSNR: 26.14, lr=0.00030\n",
            "Epoch: 2310 in 0:07:50, best epoch so far: Epoch: 2285 Training Loss: 0.002434 Mean PSNR: 26.14, lr=0.00026\n",
            "Epoch: 2320 in 0:07:52, best epoch so far: Epoch: 2285 Training Loss: 0.002434 Mean PSNR: 26.14, lr=0.00021\n",
            "Epoch: 2330 in 0:07:55, best epoch so far: Epoch: 2285 Training Loss: 0.002434 Mean PSNR: 26.14, lr=0.00016\n",
            "Epoch: 2340 in 0:07:57, best epoch so far: Epoch: 2285 Training Loss: 0.002434 Mean PSNR: 26.14, lr=0.00010\n",
            "Epoch: 2350 in 0:07:59, best epoch so far: Epoch: 2344 Training Loss: 0.002434 Mean PSNR: 26.14, lr=0.00010\n",
            "Epoch: 2360 in 0:08:01, best epoch so far: Epoch: 2344 Training Loss: 0.002434 Mean PSNR: 26.14, lr=0.00010\n",
            "Epoch: 2370 in 0:08:03, best epoch so far: Epoch: 2344 Training Loss: 0.002434 Mean PSNR: 26.14, lr=0.00010\n",
            "Epoch: 2380 in 0:08:05, best epoch so far: Epoch: 2344 Training Loss: 0.002434 Mean PSNR: 26.14, lr=0.00010\n",
            "Epoch: 2390 in 0:08:07, best epoch so far: Epoch: 2344 Training Loss: 0.002434 Mean PSNR: 26.14, lr=0.00010\n",
            "Epoch: 2400 in 0:08:09, best epoch so far: Epoch: 2344 Training Loss: 0.002434 Mean PSNR: 26.14, lr=0.00010\n",
            "Epoch: 2410 in 0:08:11, best epoch so far: Epoch: 2344 Training Loss: 0.002434 Mean PSNR: 26.14, lr=0.00010\n",
            "Epoch: 2420 in 0:08:13, best epoch so far: Epoch: 2344 Training Loss: 0.002434 Mean PSNR: 26.14, lr=0.00010\n",
            "Epoch: 2430 in 0:08:15, best epoch so far: Epoch: 2344 Training Loss: 0.002434 Mean PSNR: 26.14, lr=0.00010\n",
            "Epoch: 2440 in 0:08:17, best epoch so far: Epoch: 2344 Training Loss: 0.002434 Mean PSNR: 26.14, lr=0.00010\n",
            "No improvement for the last 100 epochs, so stopping training...\n",
            "Saving best epoch (2344) with loss: 0.002433761823900008 and psnr: 26.13721925541946 as:\n",
            "SuperResulutionNet_r-3_psnr-2614__mse-24-unknown\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.002433761823900008"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 291
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qI5xM65NBLHw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQUNCaCnBLH0",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "_lIPhVkgBLH1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "\n",
        "import PIL\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "from os import listdir\n",
        "\n",
        "# from SuperResolutionDataset import SuperResolutionDataset\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def imshow(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.figure()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "def evaluate_by_name(network_name):\n",
        "    net = torch.load(network_name)\n",
        "    evaluate(net)\n",
        "\n",
        "def mse_to_psnr(mse):\n",
        "    return 10 * math.log10(1. / mse)\n",
        "    \n",
        "def evaluate(net):\n",
        "    r = net.r\n",
        "    print(f\"r: {r}\")\n",
        "\n",
        "    use_gpu = torch.cuda.is_available()\n",
        "\n",
        "    test_set_paths = [\"test_data/\" + f for f in listdir(\"test_data\")]\n",
        "#     test_set_paths = [\"test_data/Custom\"]\n",
        "\n",
        "    for path in test_set_paths:\n",
        "        psnr = []\n",
        "        mse = []\n",
        "        test_set = SuperResolutionDataset(path, r, use_gpu=use_gpu, testing=True)\n",
        "\n",
        "\n",
        "        test_loader = torch.utils.data.DataLoader(test_set,\n",
        "                                                  batch_size=1,\n",
        "                                                  shuffle=True,\n",
        "                                                  num_workers=0)\n",
        "\n",
        "\n",
        "        for input, target in iter(test_loader):\n",
        "            if use_gpu:\n",
        "                input = input.cuda()\n",
        "                target = target.cuda()\n",
        "\n",
        "            if input.size()[1] == 1:\n",
        "                input = input.repeat(1, 3, 1, 1)\n",
        "\n",
        "            output = net(input)\n",
        "\n",
        "\n",
        "            if use_gpu:\n",
        "                input = input.cpu()\n",
        "                output = output.cpu()\n",
        "                target = target.cpu()\n",
        "\n",
        "\n",
        "            bicubic = transforms.Compose([\n",
        "                transforms.ToPILImage(),\n",
        "                transforms.Resize([int(r * input.size()[2]),\n",
        "                                   int(r * input.size()[3])],\n",
        "                                  PIL.Image.BICUBIC),\n",
        "                transforms.ToTensor()\n",
        "            ])\n",
        "            bicubic_upscaled = bicubic(input[0])\n",
        "\n",
        "            nearest_neighbour = transforms.Compose([\n",
        "                transforms.ToPILImage(),\n",
        "                transforms.Resize([int(r * input.size()[2]),\n",
        "                                   int(r * input.size()[3])],\n",
        "                                  PIL.Image.NEAREST),\n",
        "                transforms.ToTensor()\n",
        "            ])\n",
        "\n",
        "            input = nearest_neighbour(input[0])\n",
        "            # output = torch.clamp(output.detach(), 0, 1)\n",
        "\n",
        "            if target.size()[1] == 1:\n",
        "                target = target.repeat(1, 3, 1, 1)\n",
        "\n",
        "            mse_loss = nn.MSELoss()(output, target).item()\n",
        "            mse.append(mse_loss)\n",
        "            psnr.append(mse_to_psnr(mse_loss))\n",
        "\n",
        "            images = [input, target[0], output.detach()[0], bicubic_upscaled]\n",
        "\n",
        "        print(f\"{path} - avarage psnr: {np.mean(psnr)}, psnr of average mse: {mse_to_psnr(np.mean(mse))}\")\n",
        "\n",
        "    # plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "Eiknsr52BLH8",
        "colab_type": "code",
        "outputId": "842360c7-29a3-4bd1-d948-bfe29b05d970",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "evaluate(net)"
      ],
      "execution_count": 334,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "r: 3\n",
            "test_data/BSD500 - avarage psnr: 26.513302433142208, psnr of average mse: 25.305135923133733\n",
            "test_data/Set5 - avarage psnr: 29.251275389642963, psnr of average mse: 27.936438576253334\n",
            "test_data/BSD300 - avarage psnr: 26.6278637523867, psnr of average mse: 25.3310071681516\n",
            "test_data/Set14 - avarage psnr: 26.31797737340256, psnr of average mse: 25.337559041988587\n",
            "test_data/SuperTexture - avarage psnr: 24.487856278677857, psnr of average mse: 22.307540565934918\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35BnbNXBd6w9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "cedd9898-fc7c-4462-dcb0-029144cc506c"
      },
      "source": [
        "evaluate(net)"
      ],
      "execution_count": 336,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "r: 3\n",
            "test_data/BSD500 - avarage psnr: 26.494012059518397, psnr of average mse: 25.2864189330735\n",
            "test_data/Set5 - avarage psnr: 29.21752743513349, psnr of average mse: 27.90425606306598\n",
            "test_data/BSD300 - avarage psnr: 26.609591910011968, psnr of average mse: 25.31428614449313\n",
            "test_data/Set14 - avarage psnr: 26.28821186034873, psnr of average mse: 25.303261110726737\n",
            "test_data/SuperTexture - avarage psnr: 24.472527774841037, psnr of average mse: 22.293787861574557\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TefdFawqBLIF",
        "colab_type": "code",
        "outputId": "b1c5b3b8-add7-4f75-b591-e8731ab0e7a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "evaluate_by_name('SuperResulutionNet_r-3_psnr-2836__mse-15-unknown')"
      ],
      "execution_count": 288,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "r: 3\n",
            "test_data/BSD500 - avarage psnr: 26.362028775376668, psnr of average mse: 25.17812778127549\n",
            "test_data/Set5 - avarage psnr: 28.898014619497935, psnr of average mse: 27.626260223336928\n",
            "test_data/BSD300 - avarage psnr: 26.478246918008207, psnr of average mse: 25.20898656080558\n",
            "test_data/Set14 - avarage psnr: 26.091769265120707, psnr of average mse: 25.129386428463512\n",
            "test_data/SuperTexture - avarage psnr: 24.295404681202378, psnr of average mse: 22.172009585297594\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LYkaXfBBLIK",
        "colab_type": "text"
      },
      "source": [
        "### Finding Hyperparameters\n",
        "We use the AX framework to find the best hyperparameters for the network. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMeNmrySIuC6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install ax-platform "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fi2z0_YJBLIK",
        "colab_type": "code",
        "outputId": "ba3b5fad-b076-4309-ea1b-d8a9b63e213d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        }
      },
      "source": [
        "from ax import optimize\n",
        "\n",
        "def train_evaluate(parameters):\n",
        "    use_gpu = torch.cuda.is_available()\n",
        "    bs = 32\n",
        "    r = 3\n",
        "\n",
        "    training_set = SuperResolutionDataset('train_data/Set91', r, use_gpu=use_gpu)\n",
        "    # training_set = SuperResolutionDataset('test_data/BSD500', r, use_gpu=use_gpu)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        training_set, batch_size=bs, shuffle=True, num_workers=0\n",
        "    )\n",
        "\n",
        "    # Initialize the network.\n",
        "    net = SuperResolutionNet(r, activation=nn.ReLU())\n",
        "\n",
        "    return train(net, use_gpu, train_loader, r,\n",
        "                 learning_rate_factor=parameters['learning_rate_factor'],\n",
        "                 learning_rate_abs=parameters['learning_rate_abs'],\n",
        "                 print_output=False)\n",
        "\n",
        "\n",
        "best_parameters, best_values, experiment, model = optimize(\n",
        "        parameters=[\n",
        "            # {\"name\": \"lr\", \"type\": \"range\", \"bounds\": [1e-6, 0.4], \"log_scale\": True},\n",
        "            # {\"name\": \"beta1\", \"type\": \"range\", \"bounds\": [0.5, 0.9]},\n",
        "            # {\"name\": \"beta2\", \"type\": \"range\", \"bounds\": [0.9, 0.999]},\n",
        "            {\"name\": \"learning_rate_factor\", \"type\": \"range\", \"bounds\": [0.5, 1.]},\n",
        "            {\"name\": \"learning_rate_abs\", \"type\": \"range\", \"bounds\": [0., 0.0001]}\n",
        "        ],\n",
        "\n",
        "        # Booth function\n",
        "        evaluation_function=train_evaluate,\n",
        "        objective_name='training-error',\n",
        "        minimize=True,\n",
        "    )\n",
        "\n",
        "print(best_parameters, best_values, experiment, model)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO 04-18 20:34:12] ax.modelbridge.dispatch_utils: Using Bayesian Optimization generation strategy: GenerationStrategy(name='Sobol+GPEI', steps=[Sobol for 5 arms, GPEI for subsequent arms], generated 0 arm(s) so far). Iterations after 5 will take longer to generate due to model-fitting.\n",
            "[INFO 04-18 20:34:12] ax.service.managed_loop: Started full optimization with 20 steps.\n",
            "[INFO 04-18 20:34:12] ax.service.managed_loop: Running optimization trial 1...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "0.01\n",
            "0.001\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning:\n",
            "\n",
            "Couldn't retrieve source code for container of type SuperResolutionNet. It won't be checked for correctness upon loading.\n",
            "\n",
            "[INFO 04-18 20:49:34] ax.service.managed_loop: Running optimization trial 2...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "0.01\n",
            "0.001\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 04-18 21:03:48] ax.service.managed_loop: Running optimization trial 3...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "0.01\n",
            "0.001\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 04-18 21:09:51] ax.service.managed_loop: Running optimization trial 4...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "0.01\n",
            "0.001\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 04-18 21:21:22] ax.service.managed_loop: Running optimization trial 5...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "0.01\n",
            "0.001\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-2b3d6f0ce3a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mevaluation_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_evaluate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mobjective_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'training-error'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mminimize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     )\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ax/service/managed_loop.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(parameters, evaluation_function, experiment_name, objective_name, minimize, parameter_constraints, outcome_constraints, total_trials, arms_per_trial, random_seed, generation_strategy)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mgeneration_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgeneration_strategy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     )\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m     \u001b[0mparameterization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mparameterization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_current_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ax/service/managed_loop.py\u001b[0m in \u001b[0;36mfull_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Started full optimization with {num_steps} steps.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ax/service/managed_loop.py\u001b[0m in \u001b[0;36mrun_trial\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Invalid number of arms per trial: {arms_per_trial}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_trial\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ax/core/base_trial.py\u001b[0m in \u001b[0;36mfetch_data\u001b[0;34m(self, metrics, **kwargs)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \"\"\"\n\u001b[1;32m    283\u001b[0m         return self.experiment._fetch_trial_data(\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0mtrial_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m         )\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ax/core/simple_experiment.py\u001b[0m in \u001b[0;36m_fetch_trial_data\u001b[0;34m(self, trial_index, metrics, **kwargs)\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial_index\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mMetric\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     ) -> Data:\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrial_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcopy_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mExperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_tracking_metric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ax/core/simple_experiment.py\u001b[0m in \u001b[0;36meval_trial\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             evaluations[not_none(trial.arm).name] = self.evaluation_function_outer(\n\u001b[0;32m--> 124\u001b[0;31m                 \u001b[0mnot_none\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m             )\n\u001b[1;32m    126\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchTrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ax/core/simple_experiment.py\u001b[0m in \u001b[0;36mevaluation_function_outer\u001b[0;34m(self, parameterization, weight)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnum_evaluation_function_params\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;31m# pyre-fixme[20]: Anonymous call expects argument `$1`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m             \u001b[0mevaluation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluation_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameterization\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mnum_evaluation_function_params\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mevaluation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluation_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameterization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-2b3d6f0ce3a2>\u001b[0m in \u001b[0;36mtrain_evaluate\u001b[0;34m(parameters)\u001b[0m\n\u001b[1;32m     19\u001b[0m                  \u001b[0mlearning_rate_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'learning_rate_factor'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                  \u001b[0mlearning_rate_abs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'learning_rate_abs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                  print_output=False)\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-db945cc8eac3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, use_gpu, train_loader, r, learning_rate, max_epochs_without_improvement, max_epochs, print_output, momentum, learning_rate_factor, learning_rate_abs, beta1, beta2)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;31m#If the psnr is above some threshold save this new best network.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhighest_psnr\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mminimum_psnr_to_save\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'SuperResulutionNet_best_of_run-{computer_name}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mprint_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m         \u001b[0m_legacy_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_legacy_save\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    399\u001b[0m     \u001b[0mpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m     \u001b[0mpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m     \u001b[0mpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m     \u001b[0mserialized_storage_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserialized_storages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_id\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0msource_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m                 \u001b[0msource_lines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_source_lines_and_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m                 \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_lines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# saving the source is optional, so we can ignore any errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/_utils_internal.py\u001b[0m in \u001b[0;36mget_source_lines_and_file\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetsourcefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0msourcelines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_lineno\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetsourcelines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         raise OSError((\n",
            "\u001b[0;32m/usr/lib/python3.6/inspect.py\u001b[0m in \u001b[0;36mgetsourcelines\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    963\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlnum\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlnum\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetsource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/inspect.py\u001b[0m in \u001b[0;36mgetblock\u001b[0;34m(lines)\u001b[0m\n\u001b[1;32m    938\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__next__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0m_token\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m             \u001b[0mblockfinder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokeneater\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mEndOfBlock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndentationError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/tokenize.py\u001b[0m in \u001b[0;36m_tokenize\u001b[0;34m(readline, encoding)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m             \u001b[0mpseudomatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPseudoToken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpseudomatch\u001b[0m\u001b[0;34m:\u001b[0m                                \u001b[0;31m# scan for tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m                 \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpseudomatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CH4xNZimBLIQ",
        "colab_type": "code",
        "outputId": "8f8fc369-da6d-4037-c041-7e1f2181f343",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "from ax.utils.notebook.plotting import render, init_notebook_plotting\n",
        "from ax.plot.contour import plot_contour\n",
        "\n",
        "\n",
        "render(plot_contour(model=model, param_x='learning_rate_factor', param_y='learning_rate_abs', metric_name='training-error'))\n",
        "# render(plot_contour(model=model, param_x='lr', param_y='beta1', metric_name='training-error'))\n",
        "# render(plot_contour(model=model, param_x='beta1', param_y='beta2', metric_name='training-error'))\n",
        "# render(plot_contour(model=model, param_x='lr', param_y='beta2', metric_name='training-error'))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-569bb33d4e3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplot_contour\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'learning_rate_factor'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'learning_rate_abs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'training-error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# render(plot_contour(model=model, param_x='lr', param_y='beta1', metric_name='training-error'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# render(plot_contour(model=model, param_x='beta1', param_y='beta2', metric_name='training-error'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_jn9Wxmabpb",
        "colab_type": "text"
      },
      "source": [
        "Use the best parameters found by the bayesion optimization to train a network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "89135136-b08c-4673-de4e-8ad37102a04a",
        "id": "y7Agm0uKZdmn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "net = SuperResolutionNet(r, activation=nn.ReLU())\n",
        "train(net, use_gpu, train_loader, r,\n",
        "                 max_epochs=1000,\n",
        "                 max_epochs_without_improvement=100, \n",
        "                 learning_rate=best_parameters['lr'],\n",
        "                 beta1=best_parameters['beta1'],\n",
        "                 beta2=best_parameters['beta2'],\n",
        "                 )\n",
        "evaluate(net)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on gpu\n",
            "Epoch:   0 in 0.39s, best epoch so far: Epoch:   0 Training Loss: 0.088246 Mean PSNR: 10.54\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning:\n",
            "\n",
            "Couldn't retrieve source code for container of type SuperResolutionNet. It won't be checked for correctness upon loading.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 100 in 32.96s, best epoch so far: Epoch: 100 Training Loss: 0.002135 Mean PSNR: 26.71\n",
            "Epoch: 200 in 65.27s, best epoch so far: Epoch: 185 Training Loss: 0.001792 Mean PSNR: 27.47\n",
            "Epoch: 300 in 97.24s, best epoch so far: Epoch: 288 Training Loss: 0.001633 Mean PSNR: 27.87\n",
            "Epoch: 400 in 129.17s, best epoch so far: Epoch: 394 Training Loss: 0.001551 Mean PSNR: 28.10\n",
            "Epoch: 500 in 160.99s, best epoch so far: Epoch: 481 Training Loss: 0.001466 Mean PSNR: 28.34\n",
            "Epoch: 600 in 193.08s, best epoch so far: Epoch: 598 Training Loss: 0.001384 Mean PSNR: 28.59\n",
            "Epoch: 700 in 224.81s, best epoch so far: Epoch: 690 Training Loss: 0.001367 Mean PSNR: 28.64\n",
            "Epoch: 800 in 256.64s, best epoch so far: Epoch: 777 Training Loss: 0.001326 Mean PSNR: 28.77\n",
            "Epoch: 900 in 288.72s, best epoch so far: Epoch: 889 Training Loss: 0.001308 Mean PSNR: 28.84\n",
            "Saving best epoch (994) with loss: 0.0012854222453745972 and psnr: 28.9095418850852 as:\n",
            "SuperResulutionNet_r-3_psnr-2891__mse-13-unknown\n",
            "r: 3\n",
            "test_data/Set5 psnr: 30.021906725381985\n",
            "test_data/Set14 psnr: 26.845566130359053\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}