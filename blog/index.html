<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex"><!-- not public, yet ? -->
<meta name="description" content="">
<meta name="author" content="">
<link rel="icon" href="assets/img/favicon.ico">
<title>Deep learning reproducability - Superresolution </title>
<!-- Bootstrap core CSS -->
<link href="assets/css/bootstrap.min.css" rel="stylesheet">
<!-- Fonts -->
<link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Righteous%7CMerriweather:300,300i,400,400i,700,700i" rel="stylesheet">
<!-- Custom styles for this template -->
<link href="assets/css/mediumish.css" rel="stylesheet">
<style>
	figcaption {
		font-size: 60%;
	}
	</style>
		<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:black;}
.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
.tg .tg-0lax{text-align:left;vertical-align:top}
</style>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script type="text/javascript"
     src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
</head>
<body>

<!-- Begin Nav
================================================== -->
<nav class="navbar navbar-toggleable-md navbar-light bg-white fixed-top mediumnavigation">
<button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarsExampleDefault" aria-controls="navbarsExampleDefault" aria-expanded="false" aria-label="Toggle navigation">
<span class="navbar-toggler-icon"></span>
</button>
<div class="container">
	<!-- Begin Logo -->
	<a class="navbar-brand" href="index.html">
	<img src="assets/img/logo.png" alt="logo">
	</a>
	<!-- End Logo -->
	<div class="collapse navbar-collapse" id="navbarsExampleDefault">
		<!-- Begin Menu -->

	</div>
</div>
</nav>
<!-- End Nav
================================================== -->

<!-- Begin Article
================================================== -->
<div class="container">
	<div class="row">

 		<!-- Begin Fixed Left Share -->
		<div class="col-md-2 col-xs-12">

		</div> 
		<!-- End Fixed Left Share --> 

		<!-- Begin Post -->
		<div class="col-md-8 col-md-offset-2 col-xs-12">
			<div class="mainheading">

				<!-- Begin Top Meta -->
				<div class="row post-top-meta">

					<div class="col-md-12">
						<span class="author-description">This blog was made by Group 70 for the Deep Learning course of the master Computer Science at Delft University of Technology. Implementation details can be found in the <a href="https://github.com/jorisquist/DLSuperResolutionProject/blob/master/SuperResolution.ipynb">Jupyter notebook</a> in the <a href="https://github.com/jorisquist/DLSuperResolutionProject">repository</a>. Similar projects can be found on <a href="https://reproducedpapers.org">reproducedpapers.org</a>.</span>
						<span class="post-date">April 2020</span><span class="dot"></span><span class="post-read">15 min read</span>
					</div>
				</div>
				<!-- End Top Menta -->

				<h1 class="posttitle">Reproducing “Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network” </h1>

			</div>

			<!-- Begin Featured Image -->
			<img class="featured-image img-fluid" src="assets/img/header.png" alt="">
			<!-- End Featured Image -->

			<!-- Begin Post Content -->
			<div class="article-post">
				<p>
					
					As the fields of Deep Learning and Machine Learning has undergone rapid growth in the recent years, so have calls that this led to a reproducibility crisis <a href="https://science.sciencemag.org/content/359/6377/725">[1]</a>. To explore if these claims about a reproducability crisis are true and to learn from potential mistakes of recent authors, we were encouraged to do so in the course Deep Learning at the Delft University of Technology. The goal was to get some hands on experience with current methods of deep learning as well as identifying pitfalls of paper writing for our own possible future work. 
				</p>
				<p>
					We chose the paper “Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network” because of the relevance of the topic as increasing the resolution accurately can be beneficial in many applications. This technique, called super-resolution (SR) can be particularly interesting for satellite communications, surveillance, medical diagnosis, earth and astronomical observations, biometric identifications and the multimedia industry <a href="https://doi.org/10.1016/j.sigpro.2016.05.002">[2]</a><a href="https://pdfs.semanticscholar.org/d576/d9b9f941537953fd833629f8476235c7db28.pdf">[3]</a>.
				</p>
				<p>
					 The main goal is to restore a high-resolution (HR) image from the corresponding low-resolution (LR) image. This can be done with classical interpolation techniques, but maybe better performance can be achieved by using neural networks. The idea is to train a neural network to learn to upsample images. The usual difficult problem of acquiring training data or labels is easily solved by starting from a high-resolution image, classically downsampling this, and using the downsampled image as the input for a neural net, with the high-resolution image as the training set.

				<p>
					What makes this paper so special is that the way they do super-resolution is computational very effective, hence enables real-time super-resolution on 1080p videos on a single GPU. Lets get into it and explain how they do this. 
				</p>


<p>
	<b>Efficient Sub-Pixel Convolutional Neural Network (ESPCN)</b><br />
Usually super-resolution is performed by first upscaling the low-resolution image, commonly with a  bicubic interpolation, to the high-resolution space before reconstruction. The problem is that the super-resolution is performed in the high-resolution space and this is sub-optimal and adds complexity.  An example for this is the SRCNN, a model the authors seek out to improve on. The authors propose a new architecture, the “Efficient Sub-Pixel Convolutional Neural Network”. The proposed architecture has two key features: First, the L-layer convolutional neural network is directly applied to the LR space. And sub-pixel convolutional layer is only applied at the end of the network, using Periodic Shuffling (PS), to map the LR feature maps onto a SR image.  Due to the reduced input resolution smaller filter sizes can be used, lowering the computational and memory requirements. 
Secondly the upscaling filters in a L-layer network are the L-1 layers before the last one, as opposed to using on explicit upscaling filter at the end. This effectively lets the network learn a better and more complex mapping from LR to HR, increasing the accuracy of the reconstruction.
Since the periodic shuffle at the end is just a simple mapping, the reverse function can be applied to the training targets and it can be omitted for the last step of the network. This way no computation is being done on the in HR space. Figure 1 illustrates the architecture of the network.
<img class="col-md-12" src="assets/img/architecture_paper.png" /> 
</p>

<p>
	<b>Goal</b><br/>
	The goal of this project specifically is to reproduce the results from Table 1 of the paper using only information given in the paper.
	The PSNR is calculated in the following way: $PSNR = 10 \cdot log(1/MSE)$ where MSE is the mean square error between the output of the network
	and the target image. Note that PSNR has been shown to perform poorly compared to other quality metrics when it comes to estimating the quality of images as perceived by humans <a href="https://ieeexplore.ieee.org/document/4550695">[4]</a>.
	We will implementing the network using <a href="https://pytorch.org/">Pytorch</a>.
	
	<img src="assets/img/results_paper.png" class="col-md-12" />
	
	The right-most column is the best results the authors of the paper have achieved, but to do this they trained the network for a week on 50,000 images from ImageNet.
	Since that is not really practical for us to do, we are focussing on achieving the results from the "ESPCN (91 relu)" column where they used a training set of 91 images.
	Training the network on this set only takes a couple of hours, which is more within our reach.
	
</p>

<!-- implentation details -->

<p>
	<b>Implementation</b><br/>
	
	The implementation of the network itself was not very complicated using Pytorch, even the deconvolution step is included in the framework.
	
<!-- HTML generated using hilite.me --><div style="background: #f8f8f8; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><table><tr><td><pre style="margin: 0; line-height: 125%"> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28</pre></td><td><pre style="margin: 0; line-height: 125%"><span style="color: #008000; font-weight: bold">class</span> <span style="color: #0000FF; font-weight: bold">SuperResolutionNet</span>(nn<span style="color: #666666">.</span>Module):
    <span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">__init__</span>(<span style="color: #008000">self</span>, r, activation<span style="color: #666666">=</span>nn<span style="color: #666666">.</span>Identity()):
        <span style="color: #008000">super</span>()<span style="color: #666666">.</span>__init__()
        <span style="color: #008000">self</span><span style="color: #666666">.</span>r <span style="color: #666666">=</span> r
        <span style="color: #008000">self</span><span style="color: #666666">.</span>activation <span style="color: #666666">=</span> activation

        <span style="color: #008000">self</span><span style="color: #666666">.</span>layers <span style="color: #666666">=</span> nn<span style="color: #666666">.</span>ModuleList([
            nn<span style="color: #666666">.</span>Conv2d(<span style="color: #666666">3</span>, <span style="color: #666666">64</span>, <span style="color: #666666">5</span>, padding<span style="color: #666666">=2</span>),
            nn<span style="color: #666666">.</span>Conv2d(<span style="color: #666666">64</span>, <span style="color: #666666">64</span>, <span style="color: #666666">3</span>, padding<span style="color: #666666">=1</span>),
            nn<span style="color: #666666">.</span>Conv2d(<span style="color: #666666">64</span>, <span style="color: #666666">32</span>, <span style="color: #666666">3</span>, padding<span style="color: #666666">=1</span>),
        ])
        <span style="color: #008000">self</span><span style="color: #666666">.</span>last_layer <span style="color: #666666">=</span> nn<span style="color: #666666">.</span>Conv2d(<span style="color: #666666">32</span>, <span style="color: #008000">self</span><span style="color: #666666">.</span>r <span style="color: #666666">*</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>r <span style="color: #666666">*</span> <span style="color: #666666">3</span>, <span style="color: #666666">3</span>, padding<span style="color: #666666">=1</span>)
        <span style="color: #008000">self</span><span style="color: #666666">.</span>deconvolution <span style="color: #666666">=</span> nn<span style="color: #666666">.</span>PixelShuffle(<span style="color: #008000">self</span><span style="color: #666666">.</span>r)
        
        <span style="color: #008000">self</span><span style="color: #666666">.</span>l <span style="color: #666666">=</span> <span style="color: #008000">len</span>(<span style="color: #008000">self</span><span style="color: #666666">.</span>layers) <span style="color: #666666">-</span> <span style="color: #666666">1</span>  <span style="color: #408080; font-style: italic"># The number of hidden layers</span>

    <span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">forward</span>(<span style="color: #008000">self</span>, x):
        <span style="color: #008000; font-weight: bold">for</span> layer <span style="color: #AA22FF; font-weight: bold">in</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>layers:
            x <span style="color: #666666">=</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>activation(layer(x))

        <span style="color: #408080; font-style: italic"># Don&#39;t use the activation on the last convolutional layer</span>
        x <span style="color: #666666">=</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>last_layer(x)

        <span style="color: #408080; font-style: italic"># Don&#39;t apply the deconvolution step during training</span>
        <span style="color: #008000; font-weight: bold">if</span> <span style="color: #AA22FF; font-weight: bold">not</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>training:
          x <span style="color: #666666">=</span> <span style="color: #008000">self</span><span style="color: #666666">.</span>deconvolution(x)

        <span style="color: #008000; font-weight: bold">return</span> x
</pre></td></tr></table></div>


				</p>


<p>
	<b>Training</b><br/>

To start training the network, we first have to generate the data. A couple of steps have to be taken. <br />
First an input image is downscaled by a factor $r=3$. The authors of the paper also add some gaussian noise on the image before it is downscaled,
but they didn't mention exactly how they did this, so in the end we didn't do this.
Then sub-images are generated using a sliding window with size 17 and stride 14 on the downscaled images
and size 17r and 14r on the target images. This helps because then all during training all the images are the same size, so that training can be done in batches.
The reverse deconvolution is also applied to the target images so that the convolution skipped during training. This helps with performance,
because this step needs to only be done once before training instead of every forward pass during training.
<br />

The loss function used during training was the mean squared error:<br/>
<br/>
$\ell(W_{1:L}, b_{1:L}) = \frac{1}{HW}\sum^{H}_{x=1}\sum^{W}_{x=1} (\boldsymbol{I}^{HR}_{x,y} - f^L_{x,y}(\boldsymbol{I}^{LR}))^2$<br />
<br/>
This function was used since minimizing the loss is the same as maximizing the PSNR which is a good metric to meassure the performance of the upscaling.
<br/>
During training the average loss is meassured every epoch and when the loss has not improved the learning rate is lowered.
Once the loss doesn't improve for 100 epochs the training is stopped and the network is evaluated using five different data sets.

</p>

<p>
	<b>Hyper Parameter Tuning</b><br/>
	Since the paper left out quite some details about the training and hyperparameters used, we decided that we woud try to find them ourselves.
	Since we don't know which optimizer they used, we decided to use Adam. We did this because Adam can, using the right hyperparameters,
	approximate other optimizers<a href="https://openreview.net/forum?id=HygrAR4tPS">[5]</a>.
	To do this we used <a href="https://ax.dev/">AX</a>. This is a framework that uses Bayesian optimization to, among other things,
	help you find good hyperparameters for a neural network. It has a very easy to use API, which allows you to run you first optimization within a few minutes.
	
<!-- HTML generated using hilite.me --><div style="background: #f8f8f8; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><table><tr><td><pre style="margin: 0; line-height: 125%"> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15</pre></td><td><pre style="margin: 0; line-height: 125%"><span style="color: #008000; font-weight: bold">from</span> <span style="color: #0000FF; font-weight: bold">ax</span> <span style="color: #008000; font-weight: bold">import</span> optimize

best_parameters, best_values, experiment, model <span style="color: #666666">=</span> optimize(
        parameters<span style="color: #666666">=</span>[
            {<span style="color: #BA2121">&quot;name&quot;</span>: <span style="color: #BA2121">&quot;lr&quot;</span>, <span style="color: #BA2121">&quot;type&quot;</span>: <span style="color: #BA2121">&quot;range&quot;</span>, <span style="color: #BA2121">&quot;bounds&quot;</span>: [<span style="color: #666666">1e-6</span>, <span style="color: #666666">0.5</span>], <span style="color: #BA2121">&quot;log_scale&quot;</span>: <span style="color: #008000">True</span>},
            {<span style="color: #BA2121">&quot;name&quot;</span>: <span style="color: #BA2121">&quot;beta1&quot;</span>, <span style="color: #BA2121">&quot;type&quot;</span>: <span style="color: #BA2121">&quot;range&quot;</span>, <span style="color: #BA2121">&quot;bounds&quot;</span>: [<span style="color: #666666">0.</span>, <span style="color: #666666">0.999</span>]},
            {<span style="color: #BA2121">&quot;name&quot;</span>: <span style="color: #BA2121">&quot;beta2&quot;</span>, <span style="color: #BA2121">&quot;type&quot;</span>: <span style="color: #BA2121">&quot;range&quot;</span>, <span style="color: #BA2121">&quot;bounds&quot;</span>: [<span style="color: #666666">0.</span>, <span style="color: #666666">0.999</span>]},
            {<span style="color: #BA2121">&quot;name&quot;</span>: <span style="color: #BA2121">&quot;learning_rate_factor&quot;</span>, <span style="color: #BA2121">&quot;type&quot;</span>: <span style="color: #BA2121">&quot;range&quot;</span>, <span style="color: #BA2121">&quot;bounds&quot;</span>: [<span style="color: #666666">0.1</span>, <span style="color: #666666">1.</span>]},
        ],
        total_trials<span style="color: #666666">=20</span>,
        <span style="color: #408080; font-style: italic"># Function to optimize</span>
        evaluation_function<span style="color: #666666">=</span>train_evaluate,
        objective_name<span style="color: #666666">=</span><span style="color: #BA2121">&#39;training-error&#39;</span>,
        minimize<span style="color: #666666">=</span><span style="color: #008000">True</span>,
    )
</pre></td></tr></table></div><br/>


	Here 'train_evaluate' is a function that trains the network using the given parameters and returns the mean square error.
	The parameters we tried to optimize where the learning rate, the learning rate update factor, which is the factor with which the learning rate
	is multiplied when there is no performance improvement, and beta1 and 2 which are values used by the Adam optimizer.
	Running this gave us the following best values. Learning rate: 7.756e-6, beta1: 0.546, beta2: 0.281, learning_rate_factor: 0.799
	Using these values we trained one more network, which was the best performing network we found. The results for the hyperparameter optimisation can be found in the figures below.<br/>
	<img src="assets/img/hyperparameter_beta.png" class="col-md-12" />
	<img src="assets/img/hyperparameter_learn_rate.png" class="col-md-12" />
	

</p>

<p>
	<b>Results</b><br/>
	Some samples of our results can be viewed below (click for large):
	  <div class="row">
	    <div class="col-sm">
	    <figure>
	      <a href="assets/img/result_target_original.png"><img src="assets/img/result_target_original.png" /></a>
	      <figcaption>Original</figcaption>
	  	</figure>
	    </div>
	    <div class="col-sm">
	    	<figure>
	      <a href="assets/img/result_bicubic.png"><img src="assets/img/result_bicubic.png" /></a>
	      <figcaption>Downsampled, then bicubic upsampled</figcaption>
	  </figure>
	    </div>

	    	 <div class="col-sm">
	    	<figure>
	      <a href="assets/img/result_downsampled.png"><img src="assets/img/result_downsampled.png" /></a>
	      <figcaption>Downsampled</figcaption>
	  </figure>
	    </div>
	    <div class="col-sm">
	    	<figure>
	      <a href="assets/img/result_nn_output.png"><img src="assets/img/result_nn_output.png" /></a>
	      <figcaption>
	      Downsampled, then upsampled with our network
	  </figcaption>
	  </figure>
	    </div>
	</div>
<br />
The PSNR for the different dataset can be found in the table below.
<table class="tg table">
	<thead class="thead-light">
  <tr>
    <th class="tg-0pky">Dataset</th>
    <th class="tg-0pky">ESPCN (91 relu)</th>
    <th class="tg-0pky">Our implementation of <span style="font-weight:400;font-style:normal">ESPCN (91 relu)</span><br></th>
  </tr>
</thead>
  <tr>
    <td class="tg-0pky">Set5</td>
    <td class="tg-0pky">32.39</td>
    <td class="tg-0pky">30.27</td>
  </tr>
  <tr>
    <td class="tg-0pky">Set14</td>
    <td class="tg-0pky">28.97</td>
    <td class="tg-0pky">26.96</td>
  </tr>
  <tr>
    <td class="tg-0pky">BSD300</td>
    <td class="tg-0pky">28.20</td>
    <td class="tg-0pky">27.08</td>
  </tr>
  <tr>
    <td class="tg-0lax">BSD500</td>
    <td class="tg-0lax">28.27</td>
    <td class="tg-0lax">26.99</td>
  </tr>
  <tr>
    <td class="tg-0lax">SuperTexture</td>
    <td class="tg-0lax">26.38</td>
    <td class="tg-0lax">24.90</td>
  </tr>
  <tr>
    <td class="tg-0lax">Average</td>
    <td class="tg-0lax">27.76</td>
    <td class="tg-0lax">26.74</td>
  </tr>
</table>
We have succeeded in training a network that came close to the performance of the original paper, however we did not reach the level of performance that the authors did. 
</p>
<p>
<b>Discussion</b><br />
There can be several reasons why our results do no reach the same level of performance. First of all, we should note that PSNR a bit unintuitive for humans as it is a logarithmic scale and especially when comparing images from different sources this is an unreliable metric <a href="https://ieeexplore.ieee.org/document/4550695">[4]</a>. However, as we use the same dataset as the original authors we suppose this makes comparing the PSNR valid and we can now focus on some reasons why our results can deviate. 
</p>
<p>

<b>Gaussian blur</b> <br />The paper mentions using blurring in the downscaling process, however no specifics on the method and intensity of blurring are given. We have therefore not implemented blurring.<br />
<b>Optimizer</b> <br />As mentioned earlier, no specific details on the optimizer being used in the paper are given. We opted for Adam.<br/>
<b>Learning rate</b> <br />An initial learning rate and a final learning rate were given, dynamically chaning with an unspecified parameter $\mu$. We choose to do custom hyperparameter tuning based on the Ax framework.<br />
<b>Batch-size</b> <br />No details about batch sizes are given in the paper. To improve training time, we have used a batch size of 64.<br />
<b>Validation</b> <br />For the 91 images dataset, no specific details where given on dividing in a test and train set, or even if (cross-)validation was used at all.
	
</p>


<!-- End Article
================================================== -->

<div class="hideshare"></div>


<!-- Begin AlertBar
================================================== -->

<!-- End AlertBar
================================================== -->

<!-- Begin Footer
================================================== -->
<div class="container">
	<div class="footer">
		<p class="pull-left">
			 Copyright &copy; 2020 Group 70
		</p>
		<p class="pull-right">
		</p>
		<div class="clearfix">
		</div>
	</div>
</div>
<!-- End Footer
================================================== -->

<!-- Bootstrap core JavaScript
================================================== -->
<!-- Placed at the end of the document so the pages load faster -->
<script src="assets/js/jquery.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/tether/1.4.0/js/tether.min.js" integrity="sha384-DztdAPBWPRXSA/3eYEEUWrWCy7G5KFbe8fFjk5JAIxUYHKkDx6Qin1DkWx51bBrb" crossorigin="anonymous"></script>
<script src="assets/js/bootstrap.min.js"></script>
<script src="assets/js/ie10-viewport-bug-workaround.js"></script>
<script src="assets/js/mediumish.js"></script>
</body>
</html>
